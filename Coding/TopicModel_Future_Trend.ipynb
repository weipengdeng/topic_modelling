{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weipengdeng/topic_modelling/blob/main/TopicModel_Future_Trend.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hYDWLtTfAw6",
        "outputId": "c5da0f91-5418-49da-b5a4-0d1c36215da9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import gdown\n",
        "\n",
        "# Your Google Drive link\n",
        "file_id = '1O2StrVyMCiv-fwheMncKhVyfW7mWcDB3'\n",
        "url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "# Download the file first\n",
        "output = 'Master_Analysis.csv'\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "# Load the dataframe\n",
        "df = pd.read_csv(output, index_col=0)\n",
        "\n",
        "print(f\"DataFrame loaded successfully! Shape: {df.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df['Year'] == 2021]\n",
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ngs2oyIdgVz5",
        "outputId": "a3edc86e-88c9-4b2e-a8be-b2fb7d421550"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3210, 24)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "24PZFaNfgq7B",
        "outputId": "3e6cf020-5cd2-4351-a397-d53426fd2666"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Document  Dominant Topic                   Labels  \\\n",
              "3151      3151               9  urban-rural development   \n",
              "\n",
              "                    Topic_Concat                       Topic  Contribution %  \\\n",
              "3151  T9-urban-rural development  T9-urban-rural development           27.18   \n",
              "\n",
              "                                             Topic Desc  \\\n",
              "3151  development, change, growth, economic, rural, ...   \n",
              "\n",
              "                                                  Title  \\\n",
              "3151  EXTENDED URBANISATION AND THE SPATIALITIES OF ...   \n",
              "\n",
              "                                              Author  Author_Count  ...  \\\n",
              "3151  CONNOLLY, CREIGHTON;KEIL, ROGER;ALI, S. HARRIS             3  ...   \n",
              "\n",
              "     CitedReference  Year   Interval  \\\n",
              "3151            103  2021  2019-2021   \n",
              "\n",
              "                                         ResearchArea        Journal  \\\n",
              "3151  ENVIRONMENTAL SCIENCES & ECOLOGY; URBAN STUDIES  URBAN STUDIES   \n",
              "\n",
              "      CitedCount       AuthorAffiliation CorrespondingAuthorAffiliation  \\\n",
              "3151         118  UNIV LINCOLN;YORK UNIV            NATL UNIV SINGAPORE   \n",
              "\n",
              "                                Abstract_Title_Keywords  Page_Number  \n",
              "3151  THIS PAPER ARGUES THAT CONTEMPORARY PROCESSES ...         19.0  \n",
              "\n",
              "[1 rows x 24 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6a061001-4b93-44a4-82b0-7871db18d343\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Document</th>\n",
              "      <th>Dominant Topic</th>\n",
              "      <th>Labels</th>\n",
              "      <th>Topic_Concat</th>\n",
              "      <th>Topic</th>\n",
              "      <th>Contribution %</th>\n",
              "      <th>Topic Desc</th>\n",
              "      <th>Title</th>\n",
              "      <th>Author</th>\n",
              "      <th>Author_Count</th>\n",
              "      <th>...</th>\n",
              "      <th>CitedReference</th>\n",
              "      <th>Year</th>\n",
              "      <th>Interval</th>\n",
              "      <th>ResearchArea</th>\n",
              "      <th>Journal</th>\n",
              "      <th>CitedCount</th>\n",
              "      <th>AuthorAffiliation</th>\n",
              "      <th>CorrespondingAuthorAffiliation</th>\n",
              "      <th>Abstract_Title_Keywords</th>\n",
              "      <th>Page_Number</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3151</th>\n",
              "      <td>3151</td>\n",
              "      <td>9</td>\n",
              "      <td>urban-rural development</td>\n",
              "      <td>T9-urban-rural development</td>\n",
              "      <td>T9-urban-rural development</td>\n",
              "      <td>27.18</td>\n",
              "      <td>development, change, growth, economic, rural, ...</td>\n",
              "      <td>EXTENDED URBANISATION AND THE SPATIALITIES OF ...</td>\n",
              "      <td>CONNOLLY, CREIGHTON;KEIL, ROGER;ALI, S. HARRIS</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>103</td>\n",
              "      <td>2021</td>\n",
              "      <td>2019-2021</td>\n",
              "      <td>ENVIRONMENTAL SCIENCES &amp; ECOLOGY; URBAN STUDIES</td>\n",
              "      <td>URBAN STUDIES</td>\n",
              "      <td>118</td>\n",
              "      <td>UNIV LINCOLN;YORK UNIV</td>\n",
              "      <td>NATL UNIV SINGAPORE</td>\n",
              "      <td>THIS PAPER ARGUES THAT CONTEMPORARY PROCESSES ...</td>\n",
              "      <td>19.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 24 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6a061001-4b93-44a4-82b0-7871db18d343')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6a061001-4b93-44a4-82b0-7871db18d343 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6a061001-4b93-44a4-82b0-7871db18d343');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# keywords and new trend in 2021"
      ],
      "metadata": {
        "id": "hzJp1T72-TIE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## detect the emerging keywords and the keywords that regain popularity"
      ],
      "metadata": {
        "id": "CpCzzg1YHhKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "JBJzBUpGZH1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your data (assuming a CSV file with 'year' and 'keywords' columns)\n",
        "data = pd.read_csv('Master_Analysis.csv', index_col=0)\n",
        "\n",
        "def preprocess_keywords(keywords):\n",
        "    if pd.isna(keywords):\n",
        "        return []\n",
        "    processed = []\n",
        "    for kw in keywords.split(';'):\n",
        "        doc = nlp(kw.strip().lower())\n",
        "        lemmatized = \" \".join([token.lemma_ for token in doc if not token.is_stop and token.is_alpha])\n",
        "        processed.append(lemmatized)\n",
        "    return processed\n",
        "\n",
        "data['processed_keywords'] = data['KeywordsPlus'].apply(preprocess_keywords)\n",
        "data['Year'] = data['Year'].astype(int)\n",
        "\n",
        "# Split the data into two periods\n",
        "data_A = data[(data['Year'] >= 1991) & (data['Year'] <= 2020)]\n",
        "data_B = data[data['Year'] >= 2021]\n",
        "\n",
        "def get_keyword_frequencies(data):\n",
        "    all_keywords = [keyword for sublist in data['processed_keywords'] for keyword in sublist]\n",
        "    return Counter(all_keywords)\n",
        "\n",
        "freq_A = get_keyword_frequencies(data_A)\n",
        "freq_B = get_keyword_frequencies(data_B)\n",
        "\n",
        "common_keywords = set(freq_A.keys()).intersection(set(freq_B.keys()))\n",
        "emerging_keywords = {keyword: freq for keyword, freq in freq_B.items() if keyword not in common_keywords}\n",
        "regained_popularity_keywords = {keyword: freq for keyword, freq in freq_B.items() if keyword in common_keywords and freq_B[keyword] > freq_A[keyword]}\n",
        "\n",
        "# Convert to DataFrame for visualization\n",
        "emerging_keywords_df = pd.DataFrame(emerging_keywords.items(), columns=['keyword', 'frequency'])\n",
        "emerging_keywords_df = emerging_keywords_df.sort_values(by='frequency', ascending=False)\n",
        "\n",
        "regained_popularity_df = pd.DataFrame(regained_popularity_keywords.items(), columns=['keyword', 'frequency'])\n",
        "regained_popularity_df = regained_popularity_df.sort_values(by='frequency', ascending=False)\n",
        "\n",
        "# Plot the emerging keywords\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='frequency', y='keyword', data=emerging_keywords_df.head(20), palette='viridis')\n",
        "plt.xlabel('Frequency')\n",
        "plt.title('Emerging Keywords (2021 and later)')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n",
        "\n",
        "# Plot the common keywords that have regained popularity\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='frequency', y='keyword', data=regained_popularity_df.head(20), palette='plasma')\n",
        "plt.xlabel('Frequency')\n",
        "plt.title('Keywords Regaining Popularity (2021 and later)')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pKhJ1VcKHLr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Emerging keywords detection"
      ],
      "metadata": {
        "id": "e2l5CfCkQzAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Load your data (assuming a CSV file with 'year' and 'keywords' columns)\n",
        "data = pd.read_csv('Master_Analysis.csv', index_col =0)"
      ],
      "metadata": {
        "id": "wx3gJ96d-X4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoitmE2LdzGT",
        "outputId": "2c10374f-5c2e-4165-e8ec-cdd1ba5aae23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.7.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRS5aEf-eBqx",
        "outputId": "1b3c55b5-caf3-41e6-cfb5-a20fdfc49c11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-lg==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.7.1) (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2024.7.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (13.8.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.2)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.7.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nlp = spacy.load('en_core_web_lg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koJWC1cMbtNR",
        "outputId": "a76a555b-436e-45cc-a69e-90a4ccdc8739"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to preprocess keywords\n",
        "def preprocess_keywords(keywords):\n",
        "    if pd.isna(keywords):\n",
        "        return [], [], [], []\n",
        "\n",
        "    unigrams = []\n",
        "    bigrams = []\n",
        "    trigrams = []\n",
        "\n",
        "    for kw in keywords.split(';'):\n",
        "        doc = nlp(kw.strip().lower())\n",
        "\n",
        "        # Lemmatize and filter out stop words and non-alphabetic tokens\n",
        "        lemmatized = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
        "\n",
        "        # Create unigrams, bigrams, and trigrams\n",
        "        unigrams.extend(lemmatized)\n",
        "        bigrams.extend([\" \".join(lemmatized[i:i+2]) for i in range(len(lemmatized)-1)])\n",
        "        trigrams.extend([\" \".join(lemmatized[i:i+3]) for i in range(len(lemmatized)-2)])\n",
        "\n",
        "    # Combine all levels\n",
        "    combination = unigrams + bigrams + trigrams\n",
        "\n",
        "    return unigrams, bigrams, trigrams, combination\n",
        "\n",
        "# Apply the preprocessing function and create new columns for each level\n",
        "data[['unigrams', 'bigrams', 'trigrams', 'combination']] = pd.DataFrame(\n",
        "    data['Abstract_Title_Keywords'].apply(preprocess_keywords).tolist(), index=data.index\n",
        ")\n",
        "\n",
        "print(data[['unigrams', 'bigrams', 'trigrams', 'combination']].head())"
      ],
      "metadata": {
        "id": "kj34HKJC-gvP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92db9e9b-48b2-4855-88ce-bcecc754a018"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                unigrams  \\\n",
            "44030                  [subcenter, los, angeles, region]   \n",
            "44145    [spatial, mismatch, hypothesis, evidence, show]   \n",
            "43933                     [asymmetric, tax, competition]   \n",
            "43843  [paper, study, tax, competition, region, tax, ...   \n",
            "43901                     [commuting, paradox, evidence]   \n",
            "\n",
            "                                                 bigrams  \\\n",
            "44030       [subcenter los, los angeles, angeles region]   \n",
            "44145  [spatial mismatch, mismatch hypothesis, hypoth...   \n",
            "43933                  [asymmetric tax, tax competition]   \n",
            "43843  [paper study, study tax, tax competition, comp...   \n",
            "43901              [commuting paradox, paradox evidence]   \n",
            "\n",
            "                                                trigrams  \\\n",
            "44030        [subcenter los angeles, los angeles region]   \n",
            "44145  [spatial mismatch hypothesis, mismatch hypothe...   \n",
            "43933                       [asymmetric tax competition]   \n",
            "43843  [paper study tax, study tax competition, tax c...   \n",
            "43901                       [commuting paradox evidence]   \n",
            "\n",
            "                                             combination  \n",
            "44030  [subcenter, los, angeles, region, subcenter lo...  \n",
            "44145  [spatial, mismatch, hypothesis, evidence, show...  \n",
            "43933  [asymmetric, tax, competition, asymmetric tax,...  \n",
            "43843  [paper, study, tax, competition, region, tax, ...  \n",
            "43901  [commuting, paradox, evidence, commuting parad...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['Year'] = data['Year'].astype(int)"
      ],
      "metadata": {
        "id": "wx9bTWR2MKBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "hGrpRfcxZuNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install xlsxwriter\n",
        "!pip install xlsxwriter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKbJn0C3bNHs",
        "outputId": "ae5fea87-b249-407a-a56e-85c6da609a9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/159.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/159.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a list of words to remain lowercase\n",
        "lowercase_words = {\"and\", \"of\", \"on\", \"in\", \"the\", \"to\", \"with\", \"a\", \"an\", \"for\", \"at\", \"by\"}\n",
        "\n",
        "# Function to capitalize keywords appropriately\n",
        "def capitalize_keywords(keywords):\n",
        "    return ' '.join([word.capitalize() if word not in lowercase_words else word for word in keywords.split()])\n",
        "\n",
        "# List of words or phrases to filter out (case insensitive)\n",
        "filter_words = ['elsevi', 'elsevier', 'right reserve', 'ltd right', 'elsevi science', 'paper', 'describe', 'ltd', 'academic press','inc','C academic','inc right']\n",
        "\n",
        "# Function to filter out keywords containing any of the words in filter_words\n",
        "def filter_keywords(keywords):\n",
        "    # Remove keywords containing any of the words in filter_words (case insensitive)\n",
        "    return [kw for kw in keywords if not any(word in kw.lower() for word in filter_words)]\n",
        "\n",
        "# Function to calculate frequencies for a given column\n",
        "def get_keyword_frequencies(data, column_name):\n",
        "    # Flatten the list of keywords, remove empty strings, and capitalize appropriately\n",
        "    all_keywords = [\n",
        "        capitalize_keywords(keyword)\n",
        "        for sublist in data[column_name]\n",
        "        for keyword in filter_keywords(sublist)\n",
        "        if keyword.strip()  # Remove empty or whitespace-only keywords\n",
        "    ]\n",
        "    return Counter(all_keywords)\n",
        "\n",
        "# Function to convert frequencies to a DataFrame with standardized values\n",
        "def freq_to_dataframe(frequencies, top_n=200):\n",
        "    top_words = frequencies.most_common(top_n)\n",
        "    df = pd.DataFrame(top_words, columns=['Word', 'Frequency'])\n",
        "    max_frequency = df['Frequency'].max()\n",
        "    # Calculate the standardized frequency (0-100)\n",
        "    df['Standardized Frequency'] = (df['Frequency'] / max_frequency) * 100\n",
        "    return df\n",
        "\n",
        "# Create an Excel writer object\n",
        "excel_writer = pd.ExcelWriter('Result/topic_keyword_frequencies_t2.xlsx', engine='xlsxwriter')\n",
        "\n",
        "# Initialize counters for combined frequencies across all topics for each level\n",
        "combined_freq_A = {'unigrams': Counter(), 'bigrams': Counter(), 'trigrams': Counter(), 'combination': Counter()}\n",
        "combined_freq_B = {'unigrams': Counter(), 'bigrams': Counter(), 'trigrams': Counter(), 'combination': Counter()}\n",
        "combined_freq_C = {'unigrams': Counter(), 'bigrams': Counter(), 'trigrams': Counter(), 'combination': Counter()}\n",
        "combined_freq_D = {'unigrams': Counter(), 'bigrams': Counter(), 'trigrams': Counter(), 'combination': Counter()}\n",
        "combined_freq_E = {'unigrams': Counter(), 'bigrams': Counter(), 'trigrams': Counter(), 'combination': Counter()}\n",
        "combined_freq_F = {'unigrams': Counter(), 'bigrams': Counter(), 'trigrams': Counter(), 'combination': Counter()}\n",
        "\n",
        "# Loop through each unique topic\n",
        "for topic in data['Topic'].unique():\n",
        "    print(topic)\n",
        "\n",
        "    # Filter data by topic and time periods\n",
        "    data_A = data[(data['Topic'] == topic) & (data['Year'] >= 1991) & (data['Year'] <= 2000)]\n",
        "    data_B = data[(data['Topic'] == topic) & (data['Year'] >= 2001) & (data['Year'] <= 2005)]\n",
        "    data_C = data[(data['Topic'] == topic) & (data['Year'] >= 2006) & (data['Year'] <= 2010)]\n",
        "    data_D = data[(data['Topic'] == topic) & (data['Year'] >= 2011) & (data['Year'] <= 2015)]\n",
        "    data_E = data[(data['Topic'] == topic) & (data['Year'] >= 2016) & (data['Year'] <= 2018)]\n",
        "    data_F = data[(data['Topic'] == topic) & (data['Year'] >= 2019) & (data['Year'] <= 2021)]\n",
        "\n",
        "    # Calculate keyword frequencies for each level and time period\n",
        "    for level in ['unigrams', 'bigrams', 'trigrams', 'combination']:\n",
        "        freq_A = get_keyword_frequencies(data_A, level)\n",
        "        freq_B = get_keyword_frequencies(data_B, level)\n",
        "        freq_C = get_keyword_frequencies(data_C, level)\n",
        "        freq_D = get_keyword_frequencies(data_D, level)\n",
        "        freq_E = get_keyword_frequencies(data_E, level)\n",
        "        freq_F = get_keyword_frequencies(data_F, level)\n",
        "\n",
        "        # Update combined frequencies for each level\n",
        "        combined_freq_A[level].update(freq_A)\n",
        "        combined_freq_B[level].update(freq_B)\n",
        "        combined_freq_C[level].update(freq_C)\n",
        "        combined_freq_D[level].update(freq_D)\n",
        "        combined_freq_E[level].update(freq_E)\n",
        "        combined_freq_F[level].update(freq_F)\n",
        "\n",
        "        # Convert frequencies to DataFrames with standardized values\n",
        "        df_A = freq_to_dataframe(freq_A)\n",
        "        df_B = freq_to_dataframe(freq_B)\n",
        "        df_C = freq_to_dataframe(freq_C)\n",
        "        df_D = freq_to_dataframe(freq_D)\n",
        "        df_E = freq_to_dataframe(freq_E)\n",
        "        df_F = freq_to_dataframe(freq_F)\n",
        "\n",
        "        # Write each DataFrame to a separate sheet in the Excel file\n",
        "        df_A.to_excel(excel_writer, sheet_name=f'{topic.split(\"-\")[0]}_{level}_1991-2000', index=False)\n",
        "        df_B.to_excel(excel_writer, sheet_name=f'{topic.split(\"-\")[0]}_{level}_2001-2005', index=False)\n",
        "        df_C.to_excel(excel_writer, sheet_name=f'{topic.split(\"-\")[0]}_{level}_2006-2010', index=False)\n",
        "        df_D.to_excel(excel_writer, sheet_name=f'{topic.split(\"-\")[0]}_{level}_2011-2015', index=False)\n",
        "        df_E.to_excel(excel_writer, sheet_name=f'{topic.split(\"-\")[0]}_{level}_2016-2018', index=False)\n",
        "        df_F.to_excel(excel_writer, sheet_name=f'{topic.split(\"-\")[0]}_{level}_2019-2021', index=False)\n",
        "\n",
        "# Convert combined frequencies to DataFrames with standardized values and write to Excel\n",
        "for level in ['unigrams', 'bigrams', 'trigrams', 'combination']:\n",
        "    combined_df_A = freq_to_dataframe(combined_freq_A[level])\n",
        "    combined_df_B = freq_to_dataframe(combined_freq_B[level])\n",
        "    combined_df_C = freq_to_dataframe(combined_freq_C[level])\n",
        "    combined_df_D = freq_to_dataframe(combined_freq_D[level])\n",
        "    combined_df_E = freq_to_dataframe(combined_freq_E[level])\n",
        "    combined_df_F = freq_to_dataframe(combined_freq_F[level])\n",
        "\n",
        "    # Write combined DataFrames to separate sheets in the Excel file\n",
        "    combined_df_A.to_excel(excel_writer, sheet_name=f'Combined_{level}_1991-2000', index=False)\n",
        "    combined_df_B.to_excel(excel_writer, sheet_name=f'Combined_{level}_2001-2005', index=False)\n",
        "    combined_df_C.to_excel(excel_writer, sheet_name=f'Combined_{level}_2006-2010', index=False)\n",
        "    combined_df_D.to_excel(excel_writer, sheet_name=f'Combined_{level}_2011-2015', index=False)\n",
        "    combined_df_E.to_excel(excel_writer, sheet_name=f'Combined_{level}_2016-2018', index=False)\n",
        "    combined_df_F.to_excel(excel_writer, sheet_name=f'Combined_{level}_2019-2021', index=False)\n",
        "\n",
        "# Close the Excel file (saves and closes the writer)\n",
        "excel_writer.close()\n",
        "print(\"Keyword frequencies have been saved\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qo0i6YOTY-2S",
        "outputId": "6eda260d-dadb-4d56-e8d9-1f3aa7ec218b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T6-Spatial effect\n",
            "T5-Transportation\n",
            "T2-Regional economics\n",
            "T3-Socio-political geography\n",
            "T8-Environmental management\n",
            "T11-Public space and urban design\n",
            "T4-Housing and property market\n",
            "T7-Landscape and forestry\n",
            "T1-Spatial analysis and modelling\n",
            "T12-Planning policy and community governance\n",
            "T10-Neighbourhood planning\n",
            "T9-urban-rural development\n",
            "Keyword frequencies have been saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "0LtRPUEzisCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate a word cloud\n",
        "def generate_wordcloud(df, sheet_name):\n",
        "    # Create a dictionary of word frequencies\n",
        "    word_freq = dict(zip(df['Word'], df['Standardized Frequency']))\n",
        "\n",
        "    # Generate the word cloud with the shortest possible margin\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white', margin=0).generate_from_frequencies(word_freq)\n",
        "\n",
        "    # Display the word cloud\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    #plt.title(f'Word Cloud for {sheet_name}')\n",
        "\n",
        "    # Save the word cloud as an image\n",
        "    plt.savefig(f'Fig/Wordcloud/{sheet_name}.png', bbox_inches='tight')  # Save without extra white space\n",
        "    plt.close()\n",
        "\n",
        "# Read the Excel file\n",
        "excel_file = 'Result/topic_keyword_frequencies_t2.xlsx'\n",
        "xl = pd.ExcelFile(excel_file)\n",
        "\n",
        "# Generate word clouds for each sheet\n",
        "for sheet_name in tqdm(xl.sheet_names, desc=\"Generating Word Clouds\"):\n",
        "    df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
        "    generate_wordcloud(df, sheet_name)\n",
        "\n",
        "print(\"Word clouds have been generated for all sheets.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf_86pc7irPO",
        "outputId": "3a97470d-0120-4067-9398-ab6bb67dba6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Word Clouds: 100%|██████████| 312/312 [15:21<00:00,  2.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word clouds have been generated for all sheets.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}
